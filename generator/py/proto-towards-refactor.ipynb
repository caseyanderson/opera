{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "49c691aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from os import chdir, mkdir\n",
    "\n",
    "import argparse\n",
    "\n",
    "class Opera:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\" make a ghostses object\n",
    "            read the corpus into object\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        f = open(str(self.filename), 'r')\n",
    "        self.corpus = f.read() # plaintext of the corpus\n",
    "\n",
    "\n",
    "    def getSentences(self):\n",
    "        \"\"\" tokenize corpus by sentence \"\"\"\n",
    "        self.sentences = sent_tokenize(score.corpus)        \n",
    "\n",
    "    \n",
    "    def getWords(self, preserveSpaces = True):\n",
    "        \"\"\" tokenize corpus sentences by word \"\"\"\n",
    "        self.words = []\n",
    "        for sentence in self.sentences:\n",
    "            if preserveSpaces == True:\n",
    "                words = [[word_tokenize(w), ' '] for w in sentence.split()]\n",
    "                wordList = list(chain(*list(chain(*words))))\n",
    "                if wordList[-1] == ' ':\n",
    "                    # removes trailing whitespace @ end of sentence if there is any\n",
    "                    wordList.pop()\n",
    "                self.words.append(wordList)\n",
    "            if preserveSpaces == False:\n",
    "                words = word_tokenize(sentence)\n",
    "                self.words.append(words)\n",
    "        self.preserveSpaces = preserveSpaces\n",
    "\n",
    "\n",
    "    def getTagsPOS(self):\n",
    "        \"\"\" parts of speech analysis\n",
    "            TODO:\n",
    "                1. if preserveSpaces == False...\n",
    "        \"\"\"\n",
    "        self.processedCorpus = []\n",
    "        \n",
    "        if self.preserveSpaces == False:\n",
    "            print('removing spaces')\n",
    "        elif self.preserveSpaces == True:\n",
    "            # print('preserving spaces')\n",
    "            sentenceStep = 0\n",
    "            processedSentences = []\n",
    "            for sentence in self.sentences:\n",
    "                tokenizedSentence = word_tokenize(sentence)\n",
    "                posSentence = pos_tag(tokenizedSentence)                \n",
    "                processedSentence = []\n",
    "                for word in self.words[sentenceStep]:\n",
    "                    if word.isspace() == False:\n",
    "                        posWord = list(posSentence.pop(0))\n",
    "                        processedSentence.append([posWord[0], {'tagPOS': posWord[1]}])\n",
    "                    elif word.isspace() == True:\n",
    "                        processedSentence.append(word)\n",
    "                self.processedCorpus.append(processedSentence)\n",
    "                sentenceStep+=1\n",
    "                # print('~~~ no more words in sentence ~~~')\n",
    "            # print('*** no more sentences in corpus ***')\n",
    "            # print()\n",
    "\n",
    "\n",
    "    def categorizePOS(self, category, posSymbols):\n",
    "        category = category\n",
    "        posSymbols = posSymbols\n",
    "        for symbol in posSymbols:\n",
    "            sentenceCounter = 0\n",
    "            for sentence in self.processedCorpus:\n",
    "                wordCounter = 0\n",
    "                for word in sentence:\n",
    "                    if type(word) is list:\n",
    "                        if word[1]['tagPOS'] == symbol:\n",
    "                            self.processedCorpus[sentenceCounter][wordCounter][1]['categoryPOS'] = category\n",
    "                    wordCounter+=1\n",
    "                sentenceCounter+=1\n",
    "\n",
    "\n",
    "    def syleCategoriesPOS4Layer(self, category):\n",
    "        '''\n",
    "        \n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "595d0af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# setup\n",
    "score = Opera(\"corpora/Stein-short.txt\")\n",
    "score.getSentences()\n",
    "score.getWords(preserveSpaces=True)\n",
    "score.getTagsPOS()\n",
    "\n",
    "\n",
    "# make the posKeysTags dictionary\n",
    "posKeysTags={}\n",
    "categories = ['noun', 'adjective', 'verb', 'adverb', 'background', 'symbol']\n",
    "tags = [\n",
    "    ['NN','NNP','NNPS','NNS'],\n",
    "    ['JJ','JJR','JJS'],\n",
    "    ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    ['RB','RBR','RBS','WRB'],\n",
    "    ['CC','CD','DT','EX','FW','IN','LS','MD','PDT','POS','PRP','PRP$','RP','TO','UH','WDT','WP','WP$'],\n",
    "    ['$',\"''\",'(',')',',','--','.',':','SYM',\"``\"]\n",
    "]\n",
    "\n",
    "for x, y in zip(categories, tags):\n",
    "    posKeysTags[x] = y\n",
    "\n",
    "    \n",
    "# make all of the layers, output to proto dir\n",
    "for category in categories:\n",
    "    score.categorizePOS(category, posKeysTags[category])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
